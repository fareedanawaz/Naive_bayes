{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ouptput_labelling(X):\n",
    "    for row in X:\n",
    "        if row[0] == \"neutral\":\n",
    "            row[0] = '2'    \n",
    "        elif row[0] == \"positive\":\n",
    "            row[0] = '0' \n",
    "        else:\n",
    "            row[0] = '1' \n",
    "            \n",
    "    return X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fbab11b43195>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#data reading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tweets.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tweets.csv'"
     ]
    }
   ],
   "source": [
    "#data splitting\n",
    "neutral_array = []\n",
    "pos_array = []\n",
    "neg_array = []\n",
    "\n",
    "#data reading\n",
    "reader = csv.reader(open(\"Tweets.csv\", \"r\"), delimiter=\",\")\n",
    "x = list(reader)\n",
    "result = np.array(x)\n",
    "result = np.delete(result ,0 ,axis = 0)\n",
    "numrows = len(result)\n",
    "#data spliting into classes\n",
    "for row in result:\n",
    "    if row[0] == \"neutral\":\n",
    "        neutral_array.append(row)    \n",
    "    elif row[0] == \"positive\":\n",
    "        pos_array.append(row)\n",
    "    else:\n",
    "        neg_array.append(row)\n",
    "        \n",
    "        \n",
    "neutral_array = np.array(neutral_array)\n",
    "pos_array = np.array(pos_array)\n",
    "neg_array = np.array(neg_array)\n",
    "\n",
    "#splitting of actual data\n",
    "np.random.shuffle(neutral_array)\n",
    "numberofrows = (len(neutral_array)) * 0.8\n",
    "train_data_neutral = neutral_array[0:int(numberofrows),:]\n",
    "test_data_neutral = neutral_array[int(numberofrows):,:]\n",
    "\n",
    "np.random.shuffle(pos_array)\n",
    "numberofrows = (len(pos_array)) * 0.8\n",
    "train_data_pos = pos_array[0:int(numberofrows),:]\n",
    "test_data_pos = pos_array[int(numberofrows):,:]\n",
    "\n",
    "np.random.shuffle(neg_array)\n",
    "numberofrows = (len(neg_array)) * 0.8\n",
    "train_data_neg = neg_array[0:int(numberofrows),:]\n",
    "test_data_neg = neg_array[int(numberofrows):,:]\n",
    "\n",
    "train_data = np.concatenate((train_data_pos,train_data_neutral,train_data_neg))\n",
    "#print('train data lenght',len(train_data))\n",
    "#np.random.shuffle(train_data)\n",
    "test_data = np.concatenate((test_data_pos,test_data_neutral,test_data_neg))\n",
    "#print('test data length',len(test_data))\n",
    "#np.random.shuffle(test_data)\n",
    "\n",
    "train_data_string = np.delete(train_data ,0 ,axis = 1)\n",
    "#train_data_output = np.delete(train_data ,1 ,axis = 1)\n",
    "\n",
    "test_data_string = np.delete(test_data ,0 ,axis = 1)\n",
    "test_data_output = np.delete(test_data ,1 ,axis = 1)\n",
    "\n",
    "\n",
    "#separarting all classes from labels\n",
    "train_data_pos_string = np.delete(train_data_pos ,0 ,axis = 1)\n",
    "#test_data_pos_string = np.delete(test_data_pos ,0 ,axis = 1)\n",
    "train_data_neg_string = np.delete(train_data_neg ,0 ,axis = 1)\n",
    "#test_data_neg_string = np.delete(test_data_neg ,0 ,axis = 1)\n",
    "train_data_neutral_string = np.delete(train_data_neutral ,0 ,axis = 1)\n",
    "#test_data_neutral_string = np.delete(test_data_neutral ,0 ,axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_data))   \n",
    "print(len(test_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-449facef5e57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#labeling to binary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#train_data_output = ouptput_labelling(train_data_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_data_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mouptput_labelling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(train_data_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(test_data_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data_output' is not defined"
     ]
    }
   ],
   "source": [
    "#labeling to binary\n",
    "#train_data_output = ouptput_labelling(train_data_output)\n",
    "test_data_output = ouptput_labelling(test_data_output)\n",
    "#print(train_data_output)\n",
    "#print(test_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocessing(data):\n",
    "    processed_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        \n",
    "        row_lowercase = np.char.lower(row)\n",
    "        row_punctuations_removed = remove_punctuations(row_lowercase[0])\n",
    "        row_emojis_removed = remove_emojis(row_punctuations_removed) \n",
    "        row_split = row_emojis_removed.split()\n",
    "        processed_row = remove_stopWords(row_split) \n",
    "        processed_data.append(processed_row)\n",
    "    \n",
    "    return  processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing functions\n",
    "#stopwords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\",\n",
    "               \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "               \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "               \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \n",
    "               \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \n",
    "               \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "               \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
    "               \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \n",
    "               \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\",\n",
    "               \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "               \"through\", \"during\", \"before\", \"after\", \"above\", \"below\",\n",
    "               \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
    "               \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \n",
    "               \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \n",
    "               \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \n",
    "               \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
    "               \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "def remove_stopWords(string): \n",
    "    removedstring = [word for word in string if not word in stopwords]\n",
    "    return removedstring\n",
    "\n",
    "def remove_emojis(string):\n",
    "    return string.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "def remove_punctuations(string):\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return  no_punct \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary of unique word\n",
    "def findvocab(string,vocab): \n",
    "    updatedvocab = [word for word in string if not word in vocab]\n",
    "    return updatedvocab\n",
    "\n",
    "def create_vocab(data):\n",
    "    vocab = []\n",
    "    \n",
    "    for row in data:\n",
    "        newvocab = findvocab(row,vocab)\n",
    "        vocab = vocab+newvocab\n",
    "    return vocab    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_pos_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-dc646c7d3db7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#processed_train_data_string = preprocessing(train_data_string)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#processed_test_data_string = preprocessing(test_data_string)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprocessed_train_pos_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_pos_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprocessed_train_neg_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_neg_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprocessed_train_neutral_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_neutral_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_pos_string' is not defined"
     ]
    }
   ],
   "source": [
    "# preprocessing entire data and creating vocabulary\n",
    "#processed_train_data_string = preprocessing(train_data_string)\n",
    "#processed_test_data_string = preprocessing(test_data_string)\n",
    "processed_train_pos_string = preprocessing(train_data_pos_string)\n",
    "processed_train_neg_string = preprocessing(train_data_neg_string)\n",
    "processed_train_neutral_string = preprocessing(train_data_neutral_string)\n",
    "processed_train_data_string = processed_train_pos_string+processed_train_neg_string+processed_train_neutral_string\n",
    "\n",
    "vocab_result = create_vocab(processed_train_data_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6d3c51512378>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# vocab processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mupdated_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_result' is not defined"
     ]
    }
   ],
   "source": [
    "# vocab processing\n",
    "updated_vocab = vocab_result\n",
    "print(len(updated_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5f7fa4e4201b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvector_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mpos_class_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocessed_train_pos_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mneg_class_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocessed_train_neg_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mneutral_class_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocessed_train_neutral_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'updated_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "#function to return frequency of each word\n",
    "feature_vector = list()\n",
    "def create_vector(vocab,data):\n",
    "    vector_list = []\n",
    "\n",
    "    for v in vocab:\n",
    "        f=0\n",
    "        for row in data:\n",
    "            f = f+row.count(v) \n",
    "            #for add 1-normalization\n",
    "        vector_list.append(f+1)   \n",
    "    return vector_list\n",
    "  \n",
    "pos_class_vector = create_vector(updated_vocab,processed_train_pos_string)\n",
    "neg_class_vector = create_vector(updated_vocab,processed_train_neg_string)\n",
    "neutral_class_vector = create_vector(updated_vocab,processed_train_neutral_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ee1e448a093e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(type(pos_words))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneutral_class_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'updated_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "#print(type(pos_words))\n",
    "print(len(updated_vocab))\n",
    "print(len(neutral_class_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_class_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-733d3f8f41df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#frequency of words in each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpos_class_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_class_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mneg_class_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_class_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mneutral_class_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneutral_class_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'summations are'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_class_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneg_class_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneutral_class_sum\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_class_vector' is not defined"
     ]
    }
   ],
   "source": [
    "#frequency of words in each class\n",
    "pos_class_sum = sum(pos_class_vector)\n",
    "neg_class_sum = sum(neg_class_vector)\n",
    "neutral_class_sum = sum(neutral_class_vector)\n",
    "print('summations are',pos_class_sum,neg_class_sum,neutral_class_sum )\n",
    "\n",
    "#probablilities of each class\n",
    "prob_pos_class = len(train_data_pos_string)\n",
    "prob_neg_class = len(train_data_neg_string)\n",
    "prob_neutral_class = len(train_data_neutral_string)\n",
    "total_prob =  prob_pos_class+ prob_neg_class + prob_neutral_class\n",
    "#print('probabilities are ',prob_pos,prob_neg,prob_neutral,total_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input word\n",
    "#output return probability of word in each class:\n",
    "def probabilities(word):\n",
    "    index = updated_vocab.index(word)\n",
    "    return [pos_class_vector[index]/pos_class_sum,neg_class_vector[index]/neg_class_sum,neutral_class_vector[index]/neutral_class_sum]\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ce6cb485c96f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprobabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'southwestair'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-1d55df0215fb>\u001b[0m in \u001b[0;36mprobabilities\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#output return probability of word in each class:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdated_vocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpos_class_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpos_class_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneg_class_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mneg_class_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneutral_class_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mneutral_class_sum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'updated_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "probabilities('southwestair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output for single row of test data\n",
    "#first search for the word in vocab if not simply ignore (assign it to 1)\n",
    "def computeProb(data):\n",
    "    probability = [0,0,0]\n",
    "    C = [prob_pos_class/total_prob,prob_neg_class/total_prob,prob_neutral_class/total_prob]\n",
    "    #C = np.log(C)\n",
    "    for w in data:\n",
    "        if w in updated_vocab:\n",
    "            probability = probabilities(w)\n",
    "            C = [a * b for a, b in zip(C,probability)]\n",
    "            \n",
    "            \n",
    "#     find the max \n",
    "    i = np.argmax(C, axis=0)\n",
    "    \n",
    "    if i == 0:  \n",
    "        return '0'\n",
    "    elif i == 1:\n",
    "        return '1'      \n",
    "    else:\n",
    "        return '2'\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output for single row of test data\n",
    "#first search for the word in vocab if not simply ignore (assign it to 1)\n",
    "def computeProbByLogs(data):\n",
    "    probability = [0,0,0]\n",
    "    C = [prob_pos_class/total_prob,prob_neg_class/total_prob,prob_neutral_class/total_prob]\n",
    "    C = np.log(C)\n",
    "    for w in data:\n",
    "        if w in updated_vocab:\n",
    "            probability = np.log(probabilities(w))\n",
    "            C = C + probability\n",
    "            \n",
    "            \n",
    "#     find the max \n",
    "    i = np.argmax(C, axis=0)\n",
    "    \n",
    "    if i == 0:  \n",
    "        return '0'\n",
    "    elif i == 1:\n",
    "        return '1'      \n",
    "    else:\n",
    "        return '2'\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    predicted_Y = []\n",
    "    for row in data:\n",
    "        predicted_Y.append(computeProb(row))\n",
    "    return predicted_Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-543b2f1e06b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#output in processed_test_data_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprocessed_test_data_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpredicted_Y\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_test_data_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#predicted_Y  = predict(processed_test_data_string[0:4])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data_string' is not defined"
     ]
    }
   ],
   "source": [
    "#test data  \n",
    "#output in processed_test_data_string\n",
    "processed_test_data_string = preprocessing(test_data_string)\n",
    "predicted_Y  = predict(processed_test_data_string)\n",
    "#predicted_Y  = predict(processed_test_data_string[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion Matrix\n",
    "\n",
    "def confusionMatrixPos(predicted_Y, Actual_Y):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    #for positive\n",
    "    for i in range(len(Actual_Y)): \n",
    "        if predicted_Y[i] == '0' and Actual_Y[i]== '0' :\n",
    "           tp += 1\n",
    "        if predicted_Y[i]== '0' and Actual_Y[i]== '1':\n",
    "           fp += 1\n",
    "        if predicted_Y[i]== '0' and Actual_Y[i] == '2':\n",
    "           fp += 1\n",
    "        if predicted_Y[i]== '1' and Actual_Y[i] == '0' :\n",
    "           fn += 1\n",
    "          # print('A')\n",
    "        if predicted_Y[i]=='1' and Actual_Y[i] =='1' :\n",
    "           tn += 1\n",
    "           #print('B')\n",
    "        if predicted_Y[i]=='1' and Actual_Y[i] =='2' :\n",
    "           tn += 1\n",
    "           #print('C')\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='0' :\n",
    "           fn += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='1' :\n",
    "           tn += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='2' :\n",
    "           tn += 1\n",
    "\n",
    "    return(tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "\n",
    "def confusionMatrixNeg(predicted_Y, Actual_Y):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(Actual_Y)): \n",
    "        if predicted_Y[i] == '0' and Actual_Y[i]== '0' :\n",
    "           tn += 1\n",
    "        if predicted_Y[i]=='0' and Actual_Y[i]== '1':\n",
    "           fn += 1\n",
    "        if predicted_Y[i]=='0' and Actual_Y[i] == '2':\n",
    "           tn += 1\n",
    "        if predicted_Y[i]=='1' and Actual_Y[i] =='0' :\n",
    "           fp += 1\n",
    "        if predicted_Y[i]=='1' and Actual_Y[i] =='1' :\n",
    "           tp += 1\n",
    "        if predicted_Y[i]=='1' and Actual_Y[i] =='2' :\n",
    "           fp += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='0' :\n",
    "           tn += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='1' :\n",
    "           fn += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='2' :\n",
    "           tn += 1\n",
    "\n",
    "    return(tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "\n",
    "def confusionMatrixNeut(predicted_Y, Actual_Y):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(Actual_Y)): \n",
    "        if predicted_Y[i] == '0' and Actual_Y[i]== '0' :\n",
    "           tn += 1\n",
    "        if predicted_Y[i]=='0' and Actual_Y[i]== '1':\n",
    "           tn += 1\n",
    "        if predicted_Y[i]=='0' and Actual_Y[i] == '2':\n",
    "           fn += 1\n",
    "        if predicted_Y[i]== '1' and Actual_Y[i] == '0' :\n",
    "           tn += 1\n",
    "        if predicted_Y[i]== '1' and Actual_Y[i] == '1' :\n",
    "           tn += 1\n",
    "        if predicted_Y[i]== '1' and Actual_Y[i] == '2' :\n",
    "           fn += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='0' :\n",
    "           fp += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='1' :\n",
    "           fp += 1\n",
    "        if predicted_Y[i]=='2' and Actual_Y[i] =='2' :\n",
    "           tp += 1\n",
    "\n",
    "    return(tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8052bad85c0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtp0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusionMatrixPos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(tp, tn, fp, fn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtp0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfp0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfn0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtn0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Positive Matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_Y' is not defined"
     ]
    }
   ],
   "source": [
    "tp0, tn0, fp0, fn0 = confusionMatrixPos(predicted_Y,test_data_output)\n",
    "#print(tp, tn, fp, fn)\n",
    "matrix = np.array([[tp0,fp0], [fn0,tn0]])\n",
    "print('Positive Matrix')\n",
    "print(matrix)\n",
    "\n",
    "tp1, tn1, fp1, fn1 = confusionMatrixNeg(predicted_Y,test_data_output)\n",
    "matrix = np.array([[tp1,fp1], [fn1,tn1]])\n",
    "print('Negative Matrix')\n",
    "print(matrix)\n",
    "\n",
    "tp2, tn2, fp2, fn2 = confusionMatrixNeut(predicted_Y,test_data_output)\n",
    "matrix = np.array([[tp2,fp2], [fn2,tn2]])\n",
    "print('Neutral Matrix')\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tp0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-47cd4650e0e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecision0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0maccuracy0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtn0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrecall0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mF1_score0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tp0' is not defined"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "precision0 = tp0/(tp0+fp0)\n",
    "accuracy0 = (tp0+tn0)/(tp0+tn0+fp0+fn0)\n",
    "recall0 = tp0/(tp0+fn0)\n",
    "F1_score0 = ((precision0*recall0)/(precision0+recall0))*2\n",
    "print('Class Positive')\n",
    "print('precision is :',precision0)\n",
    "print('accuracy is :',accuracy0)\n",
    "print('recall is :',recall0)\n",
    "print('F1_score is :',F1_score0)\n",
    "\n",
    "print('Class Negative')\n",
    "precision1 = tp1/(tp1+fp1)\n",
    "accuracy1 = (tp1+tn1)/(tp1+tn1+fp1+fn1)\n",
    "recall1 = tp1/(tp1+fn1)\n",
    "F1_score1 = ((precision1*recall1)/(precision1+recall1))*2\n",
    "print('precision is :',precision1)\n",
    "print('accuracy is :',accuracy1)\n",
    "print('recall is :',recall1)\n",
    "print('F1_score is :',F1_score1)\n",
    "\n",
    "\n",
    "print('Class Neutral')\n",
    "precision2 = tp2/(tp2+fp2)\n",
    "accuracy2 = (tp2+tn2)/(tp2+tn2+fp2+fn2)\n",
    "recall2 = tp2/(tp2+fn2)\n",
    "F1_score2 = ((precision2*recall2)/(precision2+recall2))*2\n",
    "print('precision is :',precision2)\n",
    "print('accuracy is :',accuracy2)\n",
    "print('recall is :',recall2)\n",
    "print('F1_score is :',F1_score2)\n",
    "\n",
    "#macro\n",
    "print('Macro Measures')\n",
    "macro_precision = (precision0+precision1+precision2)/3\n",
    "macro_accuracy = (accuracy0+accuracy1+accuracy2)/3\n",
    "macro_recall = (recall0+recall1+recall2)/3\n",
    "macro_F1_score = (F1_score0+F1_score1+F1_score2)/3\n",
    "\n",
    "print('precision is :',macro_precision)\n",
    "print('accuracy is :',macro_accuracy)\n",
    "print('recall is :',macro_recall)\n",
    "print('F1_score is :',macro_F1_score)\n",
    "\n",
    "\n",
    "#micro\n",
    "print('Micro Measures')\n",
    "micro_precision  = (tp0+tp1+tp2)/(tp0+fp0+tp1+fp1+tp2+fp2)\n",
    "micro_accuracy = (tp0+tn0+tp1+tn1+tp2+tn2)/(tp0+tn0+fp0+fn0+tp1+tn1+fp1+fn1+tp2+tn2+fp2+fn2)\n",
    "micro_recall = (tp0+tp1+tp2)/(tp0+fn0+tp1+fn1+tp2+fn2)\n",
    "micro_F1_score = ((micro_precision *micro_recall)/(micro_precision+micro_recall))*2\n",
    "\n",
    "print('precision is :',micro_precision)\n",
    "print('accuracy is :',micro_accuracy)\n",
    "print('recall is :',micro_recall)\n",
    "print('F1_score is :',micro_F1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
